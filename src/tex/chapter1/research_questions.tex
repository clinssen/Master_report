\chapter{Introduction}

\section{Overview}

\todobox{
Please fix in your BibTeX file:
  
  Warning--no author, editor, or organization in nest\\
  Warning--no author, editor, or organization in cppyy\\
  Warning--no author, editor, or organization in cppyy\\
  Warning--no author, editor, or organization in cppyy\\
  Warning--no author, editor, or organization in nest\\
  Warning--no author, editor, or organization in nest\\
  Warning--no author, editor, or organization in cppyy\\
  Warning--empty year in cppyy\\
  Warning--no journal in epp\\
  Warning--no number and no volume in epp\\
  Warning--page numbers missing in epp\\
  Warning--page numbers missing in 10.3389/neuro.11.012.2008\\
  Warning--no number and no volume in nolte2002human\\
  Warning--page numbers missing in nolte2002human\\
  Warning--no author, editor, or organization in nest\\
  Warning--empty year in nest\\
  Warning--no number and no volume in plotnikov2016nestml\\
  Warning--page numbers missing in plotnikov2016nestml\\
}


The human body has always been an inspiration for many technologies. Scientists have successfully achieved to understand the mechanics behind different parts of the human body and have created models that imitate the functionalities of the studied parts. However, The human brain is one of the most complex organs in the human body \citep{nolte2002human} and we do still not fully understand its inner workings. Creating machines that can achieve human capabilities such as high-level cognition associated with conscious perception is a human dream since a long time. However, before such a goal can even remotely be achieved, we need to understand how information is processed in the brain, how that would be replicated by machines, and how to models of can be created that target specific functions of the brain and replicate their behavior in connection with other parts of the body \todo{cite some embodiment papers like https://doi.org/10.3389/fninf.2022.884180, also see the abstract for additional references}.

Successfully understanding the processes in the brain that create phenomena such as creativity, dreaming and conciousness crucially depends on efficient and easy to use \emph{neural simulators}, the development of which strongly depends on the interaction between researches from both the fields of computer science and neuroscience. Such interactions can lead to better quality and feature-richness of the simulation frameworks and as a result to a better understanding of the human mind. One building block in the neural simulation landscape are tools for the creation of neuron and synapse models which are the basic processing elements of the brain. 

\todobox{
Here needs to be a proper description of what neuons and synapses are (in a brain), how models of them work (mathematically), and  why this matters (to research). Also, there needs to be an explanation of the workflow that a usual modeler follows to get from an idea to a simulation. The distiction between network models (e.g., the result of executing a simulation script with Create() and Connect() calls) and neuron and synapse models (e.g. the basic elements, the network is created from) needs to be made clear. Best case, this includes a small PyNEST script with line numbers and a caption that describes the different parts by referring to the line numbers. 
}

The NEST Modelling Language \emph{NESTML} \todo{cite} allows users to write custom neuron and synapse models \hl{by specifying their state variables, parameters, equations and optionally a procedural description of the update.} Such models can then be used in simulation studies by loading the generated and complied model libraries at runtime into the Neural Simulation Tool \emph{NEST} \todo{cite}. In general, there are two possibilities for generating such models: The first and simple case is to generate code for neurons without knowing the synaptic connections of the model. \hl{This case only works if there are no interdependent variables between neurons and synapses}. The second and more complex case is co-generating a neuron and a synapse model together. Unfortunately\todo{Avoid emotional words like "unfortunately"}, \emph{NEST} can't \todo{always spell out these short forms: cannot} know in advance which case must be handled \todo{why not? Please explain this better}, and it must be explicitly given to \emph{NESTML} to process these configurations. Apart from these cases, the user must specify the location of the model, target platform and if needed a custom template defining how to generate the \texttt{C++} code for the given models.

\todobox{
  \textbf{This is the single most important position in your thesis!} Here, you need to explain what the problem with the old way of doing things was and why a new way is needed. Try to think like a user of NEST: You have a modelling idea and are iteratively developing the network script in PyNEST and at the same time you write your neuron and synapse models in NESTML. Then you run everything (possibly in batch mode on a supercomputer). After three hours, when your job is finally scheduled and run, you get an error message that you did not specify some neuron/synapse configuration, the path to a model, or some other stupud thing the pipeline could easily have figured out itself. This is a huge annoyance and a waste of both the users' time and possibly also expensive compute resources. Using JIT is primarily a solution to this problem. You can well spend two or three paragraphs to develop this.
}

The thesis focuses on \emph{virtually} integrating \emph{NESTML} in \emph{PyNEST} (the Python interface for \emph{NEST}). This virtual integration means that we won't modify the \emph{PyNEST} module by introducing new logic or implementing new functions, but instead we intercept the interface function calls and then decide to do some processing before or after the function call or just totally ignore the function and execute it later in the script. Mathematically described, each function $f$ in \emph{PyNEST} is replaced by one of the following execution paths:
\begin{align*}
    f \mapsto & \enspace\emptyset                                                                         \\
    f \mapsto & \enspace f \circ before                                                                   \\
    f \mapsto & \enspace f                                                                                \\
    f \mapsto & \enspace after \circ f                                                                    \\
    f \mapsto & \enspace after \circ f \circ before                                                       \\
    f \mapsto & \enspace g:\enspace \text{\#replace the call of } f \text{ with } g \in \{before, after\} \\
\end{align*}

The empty set means that we intercept the function call but do nothing. The functions \texttt{before} and \texttt{after} refer to a pre-processing and post-processing steps. It is not always the case that we have to call both functions, as we may have to call either one of them or even, like depicted in the last case, we can make the call to the function $f$ make something totally different and hide this new behavior inside one of  the processing functions. Together, the interception logic with \emph{NESTML}, the \emph{JIT} mechanism should be able to control the execution of the simulation script and decide when and which models should be instantiated and make all this decisions transparent for both, the user and the simulation script.

\todobox{
  Again use the argumentation that I put into the abstract to get from JIT to Vectorization, but go into a bit more details here.
}

Apart from virtually extending the \emph{PyNEST} interface, we adjust the \texttt{NestKernel} infrastructure to support \emph{Vectorization} \citep{nuzman2006auto}. The approach is based on modifying the data structure representing the created neurons models to a new data structure in a \emph{vector} form. This new form supports the parallel and independent processing of neurons and compared to the current representation of neurons in the \emph{NestKernel}, it should reduce the number of cache misses \citep{ghosh1997cache}, and thus we expect a speedup and performance gain during the simulation with a large network.

\section{Task Description}
My main task in the thesis is to implement a wrapper around \emph{PyNEST} to support the \emph{Just-in-Time (JIT)} mechanism. They are two main requirements that must be satisfied. The first is making the \emph{JIT} implementation as an option. It means the user can explicitly disable or enable \emph{JIT}. The second requirement is having fewer changes in the simulation script when enabling the \emph{JIT} mechanism. Thus, comparing the simulation script with and without using \emph{JIT}, we should observe minimum variations in the code.

The second challenge is extending the architecture of the \emph{NestKernel} to support a data structure of neurons in a vector form. We refer to this new architecture as \emph{Vectorization}, and it should make use of \emph{single instruction, multiple data} execution and, depending on the used models, it may reduce the number of cache misses and thus gain more speedup.
\section{Thesis Outline}


In \autoref{chap:funds}, we introduce \emph{PyNEST} and the \emph{NestKernel} with the focus on the major components involved in supporting both the \emph{JIT} and \emph{Vectorization} implementations.

\autoref{chap:jit} dives deep into understanding the requirements of the \emph{JIT} mechanism, explaining the chosen solution and comparing it with different possible approaches. Having the suggested solution explained, we point out to the design and implementation decisions. Finally, we illustrate an example how to enable \emph{JIT} in the simulation script and discuss the few changes that must be applied to the script.

\autoref{chap:vec} introduces the data structure and the necessary changes in the \emph{NestKernel} to support \emph{Vectorization}. Furthermore, we discuss the wide range of optimization features that together \emph{Vectorization} and \emph{JIT} may provide.

In \autoref{chap:perf}, the benchmark results are presented together with a set of use cases that exploits the limitations of \emph{Vectorization}.

Finally, \autoref{chap:disc} contains a discussion of the solutions that are developed in the thesis and proposes a set of additional implementation ideas that could make use of \emph{Vectorization}.



\cleardoublepage
