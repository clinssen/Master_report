\chapter{Introduction}

\section{Overview}


The human body has always been an inspiration for many technologies. Scientists have successfully achieved to understand the mechanics behind different parts of the human body and have created models that imitate the functionalities of the studied parts. However, The human brain is one of the most complex organs in the human body \citep{nolte2002human}, and we do still not fully understand its inner workings. Creating machines that can achieve human capabilities such as high-level cognition associated with conscious perception is a human dream since a long time. However, before such a goal can even remotely be achieved, we need to understand how information is processed in the brain, how that would be replicated by machines, and how models of it can be created that target specific functions of the brain and replicate their behavior in connection with other parts of the body. 

Due to technical and ethical reasons, neuroscientist may not be able to directly investigate certain parts of the brain and experiment on their responses. Therefore, to avoid experimenting on vivo and at the same time attaining realistic results, an embodied simulation framework \citep{10.3389/fninf.2022.884180} might be used to simulate the brain at scale in order to capture the contributions of multiple brain regions involved in purposeful actions.

Successfully understanding the processes in the brain that create phenomena such as creativity, dreaming and consciousness crucially depends on efficient and easy to use \emph{neural simulators}, the development of which strongly depends on the interaction between researches from both the fields of computer science and neuroscience. Such interactions may lead to better quality and feature-richness of the simulation frameworks and as a result to a better understanding of the human mind. One building block in the neural simulation landscape are tools for the creation of neuron and synapse models, which are the basic processing elements of the brain. 


The nervous system consists mainly of two types of cells: \emph{neuron} and \emph{glia}. The Neurons are information messengers. They use electrical impulses and chemical signals to transmit information between different areas of the brain and between the brain and the rest of the nervous system. Everything we \emph{think}, \emph{feel} and \emph{do} would be impossible without the work of neurons and their support cells. Glial cells provide support for neurons. For example, they provide neurons with nutrients and other materials.


\begin{figure}[ht!]
  \centering
  \includegraphics[width=\linewidth]{src/pic/neuron.eps}
  \caption{The neuron model from \citep{introductiontopsychology}: The neuron consists of three major components. The cell body also
  known as the soma is the core of the neuron containing the nucleus of the cell and keeping
  it alive. The Dendrites are a tree-like branching fiber collecting information from the other
  cells and sending them to the soma. The Axon is a long-segmented fiber transmitting
  information away from the cell body toward other connected neurons. Moreover, the axon
  is surrounded by a layer of fatty tissue knows as the meylin sheath. It acts as insulator and
  allows faster transmission of the electrical signal by preventing the electrical charge from
  shorting out.}
  \label{fig:neuron}
\end{figure}


\begin{figure}[ht!]
  \centering
  \includegraphics[width=\linewidth]{src/pic/synaptic.eps}
  \caption{The Synapse from \citep{Neurotransmitters}: Most communication between neurons occurs at a specialized structure called a synapse. Synapse is the area where two neurons come close enough to one another that they are able to pass a chemical signals (Neurotransmitters) from once to another. The synaptic gap between the neurons is called the synaptic cleft. The Neurotransmitters are packaged into small sacs called vesicles (blue circles in the figure). Each vesicle can contain thousands of neurotransmitter molecules. When the presynaptic neuron fires, it causes the vesicles to fuse with the presynaptic membrane and release the neurotransmitter molecules into the cleft. Once the neurotransmitter are in the synaptic cleft, they interact with the receptors on the postsynaptic membrane. They bind with them and may cause an action to occur in the postsynaptic cell.}
  \label{fig:synapse}
\end{figure}


The whole process of communication between neurons as depicted in \autoref{fig:neuron} is based on passing an \emph{electrochemical} signal from one-to-many. An electrical impulse travels through the neuron and a chemical reaction is triggered to transmit the received signal between the neurons. By receiving a signal at the \emph{dendrites} level, the signal is transmitted to the \emph{soma} in the form of an electrical signal. If the accumulated signal is strong enough, it may be forwarded to the \emph{axon} and afterwards to the terminal stations. Reaching the terminal stations, a chemical reaction takes place emitting \emph{ions} knows as the \emph{neurotransmitters}, which are transported to the other connected neurons across the space between the cells. This spacing is also known as \emph{synapses}, as shown in \autoref{fig:synapse}. 

We distinguish three states or phases that a neuron may undergo, the \emph{resting potential, action potential } and \emph{hyper-polarization}.
In the \emph{resting potential } phase, the \emph{axon} remains in a state in which the interior of the cell contains a greater number of negatively charged ions than the outside of the cell.
Having the first segment of the \emph{axon} stimulated by the electrical signal from the \emph{dendrites} and if the impulse is strong enough that it goes beyond a certain \emph{threshold}, the first segment opens its gates allowing the positively charged sodium $Na^{+}$ to enter the cell. This change in electrical charge is known as the \emph{action potential}. Once the action potential occurs, the number of positive ions exceeds the number of negative ions in the segment, and it becomes temporarily positively charged. As a consequence of the \emph{action potential} occurring on the first segment of the \emph{axon}, a similar electrical change occurs in the next segment, and thus the electrical impulse continues travelling the \emph{axon} until reaching the terminal stations. It is important to mention, when the signal passes the next segment, the prior segment's gate closes again and returns to its negative state. Another important aspect of the neuron's phases that it cannot be partial. It is that the neuron either \emph{fires} completely, such that the action potential travels all the way down to the axon, or it does not fire at all. When the neuron \emph{fires}, it provides more energy to neurons down the line by firing faster or by enhancing the synapse of the postsynaptic neuron. After firing, the neuron does not instantly fire again if other electrical impulses are being accumulated from the received signals at the dendrites, but it takes a brief time after firing waiting for the resting potential to be established again. This phase is known as the \emph{hyper-polarization}, and it prevents the neuron to form instantly and repeatedly firing again.


Having the electrical signal reaching the terminal stations, the electrical impulse signals them to release the neurotransmitters into the synapse. The neurotransmitters travel then across the synaptic space between the terminal stations of one neuron and the dendrites of the others, where they bind to the dendrites in the postsynaptic neurons. Neurons may have the property of either having an \emph{excitatory} or \emph{inhibitory } effect to all its postsynaptic neurons. Neurotransmitters exhibiting an excitatory effect tend to make the neuron more likely to fire. However, in the case of inhibitory neurotransmitters, they make the cell less likely to fire. Given that different neurotransmitters arrive at the dendrites, the neuron will be influenced by the excitatory and inhibitory signals. The neuron accumulates in that cases both type of signals and when the total effect of the excitatory signals are  greater than the total effect of the inhibitory signals, the neuron moves closer to its firing threshold. Reaching the threshold, the action potential is triggered and the process of transmitting the signal to the other neurons begins.

The overall complexity of the brain, as well as the tremendous number of neurons and their synaptic connections,
require an efficient framework to simulate the targeted parts in the brain and to efficiently manage the available resources in the framework environment. The Python module \citep[\emph{PyNEST};][]{10.3389/neuro.11.012.2008} that runs the \emph{neural simulation tool NEST} \citep{Gewaltig:NEST, spreizer_sebastian_2022_6368024} in the backend provides such framework capable of simulating large networks with different neuron and synapse models. NEST supports a variety of neurons and synapses models. Neurons may be anything from \emph{simple point} neuron models like the \emph{integrate-and-fire} neuron to complex compartmental neurons. On the other hands, synapses may either be \emph{static} or have a variant weight that changes over time according to a \emph{plasticity} algorithm. NEST supports a variety of algorithms for synapses. Such algorithms are spike-timing-dependent plasticity (STDP), short-term plasticity (STP), or third-factor neuromodulated weight dynamics.

The neural network in NEST is generated from neurons and their connections using the essential high level API functions in PyNEST. The \texttt{Create} function is responsible for creating instances of the desired neuron models, such models may be the \emph{integrate-and-fire (IAF)}  with current- and conductance based synapses. The \texttt{Connect} function is responsible for connecting the neurons and shaping the topology of the neural network. The type of the synapse model used between the pre-synaptic neuron and the post-synaptic neuron is also defined in the \texttt{Connect} function.  \autoref{lst:simple_simulation} shows a simple example how to create two neurons and connect them using a specific synapse type.
  

\begin{figure}[ht!]
  \centering
  \begin{lstlisting}[language=Python, label=lst:simple_simulation, caption={Every simulation script using the PyNESt module starts with impoting the \texttt{nest} module in Python. In the fourth line, we create two Hodgkin-Huxley neurons as  instance of the \emph{hh\_psc\_alpha\_gap} model. In the fifth line, we set both neurons to recieve a constant current of 100.0 pA. In the sixth line, we only modifiy the membrane potentials of the first neuron instance and set it to -10. In the eighth line, we create a voltmeter to record the changes in the membrane potential for both neurons. In the eleventh line, we connect the first neuron with the second neuron, and vice-versa. The \emph{gap\_junction} synapse model is used in the created connections between the first and second neuron. Finlay, we connect the  voltmeter to the neurons to activlay record the changes in the membrane potential. In the last line, we call the \texttt{Simulate} function to start the simulation for 10 miliseconds.}, captionpos=b]
  import nest
 
  # Create neurons and devices
  neurons = nest.Create('hh_psc_alpha_gap', 2)
  neurons.I_e = 100.
  neurons[0].V_m = -10.

  vm = nest.Create('voltmeter', params={'interval': 0.1})

  # Connecting the created neurons with gap_junction synapse model
  nest.Connect(neurons, neurons,
             {'rule': 'all_to_all', 'allow_autapses': False},
             {'synapse_model': 'gap_junction', 'weight': 0.5})
  
  
  nest.Connect(vm, neurons, 'all_to_all')

  # Simulate the neural network
  nest.Simulate(10.)
  \end{lstlisting}
  %\caption{The Simulation script imports without \emph{JIT}}
  %\label{fig:imports_without_jit}
  \end{figure}

Users are not restricted to only the available models in NEST. They may also create custom neuron and synapse models by using the  NEST Modelling Language \emph{NESTML} \citep{plotnikov2016nestml, linssen_charl_a_p_2022_5784175}. NESTML allows users to write custom neuron and synapse models by specifying their state variables, parameters, equations and optionally a procedural description of how the inner state of the neuron might be updated during the simulation. Such models can then be used in simulation studies by loading the generated and complied model libraries at runtime into the Neural Simulation Tool \emph{NEST} \citep{spreizer_sebastian_2022_6368024}. 

In general, there are two possibilities for generating such models: The first and simple case is to generate code for neurons without specifying the synapse model. This case only works if there are no interdependent variables between neurons and synapses. The second and more complex case is co-generating a neuron and a synapse models together. Each custom written model in NESTML is defined by its name in the \emph{neuron} block in the language. The specified name in the main \emph{neuron} block is used for generating the library code in C++ and for using the model in the simulation script.


In \autoref{lst:nestml_without_synapse} and \autoref{lst:nestml_with_synapse}, we use the \texttt{generate\_nest\_target} function to generate the code of the extension module. In  \autoref{lst:nestml_without_synapse}, we only generate a neuron model without a synapse dependency. The output of the code generation pipeline is a dynamic library that can be installed during the simulation in NEST. A required and essential parameter for the code generation pipeline is the location of the NESTML model, and it is defined by the \texttt{input\_path} in the {generate\_nest\_target} function. After installing the library, an instance of the model can be created using the \texttt{Create} function and connected with the other elements in the network using the \texttt{Connect} function. If the created instance of the model is a presynaptic node, then it can be connected with any \emph{built\_in} synapse in NEST without any preconditions that must be satisfied. In contrast to \autoref{lst:nestml_without_synapse}, the neuron model used in \autoref{lst:nestml_with_synapse} is co-generated with a synapse model. The co-code generation of neuron and synapse models generates in one part the same neuron model as in  \autoref{lst:nestml_without_synapse} and then another neuron model that has the same properties with extended functionalities for supporting the provided synapse model, and of course the synapse model code. To distinct between the two generated neuron models in the presence of a synapse model, the code generation pipeline modifies the name of the model that supports the synapse by including the synapse's name in the neuron's name and the same happens to the synapse model. In short, both  the names of the neuron and synapse model include the name of each other, and thus the user should be aware which model type should be used when calling the \texttt{Connect} function with the custom implemented synapse model.

\begin{figure}[ht!]
  \centering
  \begin{lstlisting}[language=Python, label=lst:nestml_without_synapse, caption={Generating extension module code: The \texttt{generate\_nest\_target} function  generates code only of a neuron model. The minimum required parameter of the function is the \texttt{input\_path} that points to the location of the model. Once the code is generated, the built libraries can be loaded using the \texttt{Install} function by providing the \emph{simple\_module} as the name of the generated library. Is the model installed in NEST, we can create an instance of the model by calling the \texttt{Create} function with the model name being the name that was writen in the NESTML file in the \emph{neuron} block.}, captionpos=b]
  # generating code for neurons only
  generate_nest_target(input_path="neuron_model.nestml",
                      target_path="where_to_generate_the_code",
                      module_name="simple_module",
                      logging_level="ERROR")

  # Installing the extension module
  nest.Install("simple_module")

  # Create Instance of the model
  neuron = nest.Create("neuron_model", 2)

  # Create network
  nest.Connect(neurons[0], neurons[1],  syn_spec={'synapse_model': "any_built_in_synapse_type"})
  \end{lstlisting}
  \end{figure}

\begin{figure}[ht!]
  \centering
  \begin{lstlisting}[language=Python, label=lst:nestml_with_synapse, caption={The \texttt{generate\_nest\_target} function generates code for the neuron and synapse together. The \texttt{input\_path} takes a list of nestml files with the first being the neuron model and the second the synapse model. Additionally, the library will be generated with the \emph{complex\_module} as the library name. In contrast to the first case, co-generating the code for the neuron and synapse together requires providing the \texttt{codegen\_opts} that is responsible for specifying the realtion between the neuron and the syanpse. Installing the new library is not different than the previous case, we simply call the \texttt{Install} function with the \emph{complex\_moduel} as the library name. The only difference now is creating instance of the neuron model. The neuron model is no longer registered in NEST under the same name as \emph{"neuron\_model}, but due to co-generating the neuron with the synapse, the neuron has more logic that handles its relation with the synapse model. The new name of the neuron is computed as the concatenation of its name with the synapse name, and it is available as \emph{neuron\_model\_\_with\_synapse\_model"}. The same happens to the synapse model and it is only available as \emph{synapse\_model\_\_with\_neuron\_model"}. This dependency forces the connection type in the network. Is the  \emph{synapse\_model\_\_with\_neuron\_model"} the synapse model involved in the connection, then the only valid postsynapse neuron model type is the \emph{neuron\_model\_\_with\_synapse\_model"}. Otherwise NEST will throw an exception and quit excuting the simulation script.}, captionpos=b]
  # generating code for neuron with a synapse
  generate_nest_target(input_path=["neuron_model.nestml", "synapse_model.nestml"],
                      target_path="where_to_generate_the_code",
                      logging_level="ERROR",
                      module_name="complex_module",
                      codegen_opts=
                      {
                      "neuron_parent_class": "StructuralPlasticityNode",
                      "neuron_parent_class_include": "structural_plasticity_node.h",
                      "neuron_synapse_pairs":
                      [
                        {
                        "neuron": "neuron_model",
                        "synapse": "synapse_model"
                        }
                      ]})
  
  
  # Installing the extension module
  nest.Install("complex_module", 2)
                                                            
  # Create Instance of the model
  neurons = nest.Create("neuron_model__with_synapse_model")

  # Create network
  nest.Connect(neurons[0], neurons[1],  syn_spec={'synapse_model': "synapse_model__with_neuron_model"})

  \end{lstlisting}
  \end{figure}


  Installing new libraries and using the correct neuron type in the \texttt{Connect} function with a custom synapse model might be a bit aggravating. The reason behind it is that the library name does not explicitly imply the model's names that are built into the library, and if the user forgot which library holds the required model, he might need to trigger the code generation pipeline again. Another reason is using an existing library. The user must explicitly configure a certain environment variable to specify the location of the library to NEST, and then call the \texttt{Install} function on the libraries he may want to use. Finally,  co-generating a neuron model with a synapse, the user must be aware of the naming change and accordingly use the correct model type in the \texttt{Connect} function, otherwise NEST will throw an error indicating that the chosen synapse type supports only a certain neuron model type.
  
  Mostly, building the network is one of the last steps in the simulation script, and before the user might create any arbitrary number of instances of any available model and configure their initial state with random values. Between creating these instances and connecting them, the user might have some function calls in between that may take certain amount of time and only when reaching the \texttt{Connect} calls, the execution of the simulation script stops because of the given parameters in the function are irreconcilable. This problem may definitely occur when the custom installed synapse model does not support the type of the postsynapse neuron model type.  This problems might happen over and over during modelling the neural network in NEST, and it can be indirectly accounted for the overall simulation time of the configured network, as the user must manually manage these failures and fix them separately.

The focus of the thesis is to have a fully automated framework for generating external models in PyNEST and to conceal the usage of the \texttt{Install} function and the naming change of the neuron model when co-generating with a synapse model. The automated framework should \emph{virtually} integrate \emph{NESTML} in \emph{PyNEST} (the Python interface for \emph{NEST}). This virtual integration means that we won't modify the \emph{PyNEST} module by introducing new logic or implementing new functions, but instead we intercept the interface function calls and then decide to do some processing before or after the function call or just totally ignore the function and execute it later in the script. Mathematically described, each function $f$ in \emph{PyNEST} is replaced by one of the following execution paths:
\begin{align*}
    f \mapsto & \enspace\emptyset                                                                         \\
    f \mapsto & \enspace f \circ before                                                                   \\
    f \mapsto & \enspace f                                                                                \\
    f \mapsto & \enspace after \circ f                                                                    \\
    f \mapsto & \enspace after \circ f \circ before                                                       \\
    f \mapsto & \enspace g:\enspace \text{\#replace the call of } f \text{ with } g \in \{before, after\} \\
\end{align*}

The empty set means that we intercept the function call but do nothing. The functions \texttt{before} and \texttt{after} refer to a pre-processing and post-processing steps. It is not always the case that we have to call both functions, as we may have to call either one of them or even, like depicted in the last case, we can make the call to the function $f$ make something totally different and hide this new behavior inside one of  the processing functions. Together, the interception logic with \emph{NESTML}, the \emph{JIT} mechanism should be able to control the execution of the simulation script and decide when and which models should be instantiated and make all this decisions transparent for both, the user and the simulation script.

Before connecting instances of the available models  and starting the simulation, the user may modify the inner attributes of the instances by assigning either deterministic or random values. However, these attributes are only available after the complete code generation of the model's library. With JIT, we are able to retrieve these attributes and create a partial version of the model. However, this solution comes with the cost of doubling the memory requirements. The reason behind it is that we cache the partial version on the Python side until the real instances are available by calling either the \texttt{Connect} or \texttt{Simulate} functions.

To abate the memory requirements between the Python level and the C++ level, we adjust the \texttt{NestKernel} infrastructure to support \emph{Vectorization} \citep{nuzman2006auto}. The approach is based on modifying the data structure representing the created neurons models to a new data structure in a \emph{vector} form. This new form supports the parallel and independent processing of neurons and compared to the current representation of neurons in the \emph{NestKernel}, it should reduce the number of cache misses \citep{ghosh1997cache}, and thus we expect a speedup and performance gain during the simulation with a large network.

\section{Task Description}
My main task in the thesis is to implement a wrapper around \emph{PyNEST} to support the \emph{Just-in-Time (JIT)} mechanism. They are two main requirements that must be satisfied. The first is making the \emph{JIT} implementation as an option. It means the user can explicitly disable or enable \emph{JIT}. The second requirement is having fewer changes in the simulation script when enabling the \emph{JIT} mechanism. Thus, comparing the simulation script with and without using \emph{JIT}, we should observe minimum variations in the code.

The second challenge is extending the architecture of the \emph{NestKernel} to support a data structure of neurons in a vector form. We refer to this new architecture as \emph{Vectorization}, and it should make use of \emph{single instruction, multiple data} execution and, depending on the used models, it may reduce the number of cache misses and thus gain more speedup.
\section{Thesis Outline}


In \autoref{chap:funds}, we introduce \emph{PyNEST} and the \emph{NestKernel} with the focus on the major components involved in supporting both the \emph{JIT} and \emph{Vectorization} implementations.

\autoref{chap:jit} dives deep into understanding the requirements of the \emph{JIT} mechanism, explaining the chosen solution and comparing it with different possible approaches. Having the suggested solution explained, we point out to the design and implementation decisions. Finally, we illustrate an example how to enable \emph{JIT} in the simulation script and discuss the few changes that must be applied to the script.

\autoref{chap:vec} introduces the data structure and the necessary changes in the \emph{NestKernel} to support \emph{Vectorization}. Furthermore, we discuss the wide range of optimization features that together \emph{Vectorization} and \emph{JIT} may provide.

In \autoref{chap:perf}, the benchmark results are presented together with a set of use cases that exploits the limitations of \emph{Vectorization}.

Finally, \autoref{chap:disc} contains a discussion of the solutions that are developed in the thesis and proposes a set of additional implementation ideas that could make use of \emph{Vectorization}.



\cleardoublepage
